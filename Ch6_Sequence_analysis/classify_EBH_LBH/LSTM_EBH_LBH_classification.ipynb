{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of EBH and LBH clauses with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this script a model is trained on clauses from EBH and LBH books. With this model one can classify clauses as either EBH or LBH. This is done on clauses from Jonah, Ruth and the prose tale of Job.\n",
    "\n",
    "The model uses a so called Long Short Term Memory network (or LSTM network), which is capable of finding complex patterns in sequence data.\n",
    "\n",
    "In this script, the model is trained on all EBH and LBH books, with the exception of one book. This is the validation book, on which the model is validated. All the books are the validation book, one after the other. For each validation book, the model is trained 200 times. The results may vary, due to sampling variation and variation in the initialization of the weights. The final result is the average of 200 runs of the model.\n",
    "\n",
    "Finally, the clauses from Jonah, the prose tale of Job, and Ruth are classified as EBH or LBH, to find out whether the language of these texts is more similar to EBH or LBH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to analyze data on phrase level or word level. In the phrase level analysis, the clause is represented as a sequence of phrase functions. In the word level analysis, the clause is represented as a sequence of parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinction is made between narrative (N) and quoted speech (Q)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose what you want to analyze in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level should be 'phrase_level' or 'word_level'\n",
    "\n",
    "level = 'phrase_level'\n",
    "\n",
    "#txt_type should be 'Q' or 'N'\n",
    "\n",
    "txt_type = 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, csv, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start TF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tconnecting to online GitHub repo annotation/app-bhsa ... connected\n",
      "Using TF-app in C:\\Users\\geitb/text-fabric-data/annotation/app-bhsa/code:\n",
      "\trv1.2=#5fdf1778d51d938bfe80b37b415e36618e50190c (latest release)\n",
      "\tconnecting to online GitHub repo etcbc/bhsa ... connected\n",
      "Using data in C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c:\n",
      "\trv1.6=#bac4a9f5a2bbdede96ba6caea45e762fe88f88c5 (latest release)\n",
      "\tconnecting to online GitHub repo etcbc/phono ... connected\n",
      "Using data in C:\\Users\\geitb/text-fabric-data/etcbc/phono/tf/c:\n",
      "\tr1.2 (latest release)\n",
      "\tconnecting to online GitHub repo etcbc/parallels ... connected\n",
      "Using data in C:\\Users\\geitb/text-fabric-data/etcbc/parallels/tf/c:\n",
      "\tr1.2 (latest release)\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Documentation:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Writing/Hebrew\" title=\"('Hebrew characters and transcriptions',)\">Character table</a> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a> <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa API documentation\">bhsa API</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/\" title=\"text-fabric-api\">Text-Fabric API 7.9.0</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Use/Search/\" title=\"Search Templates Introduction and Reference\">Search Reference</a><details open><summary><b>Loaded features</b>:</summary>\n",
       "<p><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b>: <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/book.tf\">book</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/book@am.tf\">book@ll</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/chapter.tf\">chapter</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/code.tf\">code</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/det.tf\">det</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/domain.tf\">domain</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/freq_lex.tf\">freq_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/function.tf\">function</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/g_cons.tf\">g_cons</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/g_cons_utf8.tf\">g_cons_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/g_lex.tf\">g_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/g_lex_utf8.tf\">g_lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/g_word.tf\">g_word</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/g_word_utf8.tf\">g_word_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/gloss.tf\">gloss</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/gn.tf\">gn</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/label.tf\">label</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/language.tf\">language</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/lex.tf\">lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/lex_utf8.tf\">lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/ls.tf\">ls</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/nametype.tf\">nametype</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/nme.tf\">nme</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/nu.tf\">nu</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/number.tf\">number</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/otype.tf\">otype</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/pargr.tf\">pargr</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/pdp.tf\">pdp</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/pfm.tf\">pfm</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/prs.tf\">prs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/prs_gn.tf\">prs_gn</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/prs_nu.tf\">prs_nu</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/prs_ps.tf\">prs_ps</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/ps.tf\">ps</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/qere.tf\">qere</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer.tf\">qere_trailer</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer_utf8.tf\">qere_trailer_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/qere_utf8.tf\">qere_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/rank_lex.tf\">rank_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/rela.tf\">rela</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/sp.tf\">sp</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/st.tf\">st</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/tab.tf\">tab</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/trailer.tf\">trailer</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/trailer_utf8.tf\">trailer_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/txt.tf\">txt</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/typ.tf\">typ</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/uvf.tf\">uvf</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/vbe.tf\">vbe</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/vbs.tf\">vbs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/verse.tf\">verse</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/voc_lex.tf\">voc_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/voc_lex_utf8.tf\">voc_lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/vs.tf\">vs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/vt.tf\">vt</a>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/mother.tf\">mother</a></i></b>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c/oslots.tf\">oslots</a></i></b> </p><p><b>Parallel Passages</b>: <b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/parallels/tf/c/crossref.tf\">crossref</a></i></b> </p><p><b>Phonetic Transcriptions</b>: <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/phono/tf/c/phono.tf\">phono</a>  <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/phono/tf/c/phono_trailer.tf\">phono_trailer</a> </p></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src:\n",
       "    local(\"SILEOT.ttf\"),\n",
       "    url(\"https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true\");\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0a6611;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".verse {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".vl {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".sentence,.clause,.phrase {\n",
       "    margin-top: -1.2em;\n",
       "    margin-left: 1em;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3em;\n",
       "    border-style: solid;\n",
       "    border-radius: 0.2em;\n",
       "    font-size: small;\n",
       "    display: block;\n",
       "    width: fit-content;\n",
       "    max-width: fit-content;\n",
       "    direction: ltr;\n",
       "}\n",
       ".atoms {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".satom,.catom,.patom {\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    border-radius: 0.3em;\n",
       "    border-style: solid;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".sentence {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".clause {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".phrase {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".satom {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 4px;\n",
       "}\n",
       ".catom {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".patom {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".word {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 1px solid #cccccc;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".lextp {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 2px solid #888888;\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       ".satom.l,.catom.l,.patom.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".satom.r,.catom.r,.patom.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".satom.lno,.catom.lno,.patom.lno {\n",
       "    border-left-style: none\n",
       "}\n",
       ".satom.rno,.catom.rno,.patom.rno {\n",
       "    border-right-style: none\n",
       "}\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".prb,.prb a:visited,.prb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".h,.h a:visited,.h a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".hb,.hb a:visited,.hb a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    line-height: 2;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".vn {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       ".rela,.function,.typ {\n",
       "    font-family: monospace;\n",
       "    font-size: small;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".pdp,.pdp a:visited,.pdp a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".voc_lex {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vs {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vt {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".gloss {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #444444;\n",
       "}\n",
       ".vrs {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #444444;\n",
       "}\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "\n",
       "tr.tf, td.tf, th.tf {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       "span.hldot {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder: 0.2rem solid var(--hl-rim);\n",
       "\tborder-radius: 0.4rem;\n",
       "\t/*\n",
       "\tdisplay: inline-block;\n",
       "\twidth: 0.8rem;\n",
       "\theight: 0.8rem;\n",
       "\t*/\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "\n",
       "span.hlup {\n",
       "\tborder-color: var(--hl-dark);\n",
       "\tborder-width: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.2rem;\n",
       "  padding: 0.2rem;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55, 100%,  60%, 0.9  );\n",
       "\t--hl-dark:          hsla( 55, 100%,  40%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<details open><summary><b>API members</b>:</summary>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">C Computed</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">Call AllComputeds</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">Cs ComputedString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">E Edge</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">Eall AllEdges</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">Es EdgeString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">ensureLoaded</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">TF</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">ignored</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">loadLog</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Locality/#locality\" title=\"doc\">L Locality</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">cache</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">error</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">indent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">info</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">isSilent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">reset</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">setSilent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">silentOff</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">silentOn</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">warning</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">N Nodes</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortKey</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortKeyTuple</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">otypeRank</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortNodes</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">F Feature</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">Fall AllFeatures</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">Fs FeatureString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Search/#search\" title=\"doc\">S Search</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Text/#text\" title=\"doc\">T Text</a></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EBH and LBH subcorpora are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebh = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', 'Samuel', 'Kings']\n",
    "lbh = ['Esther', 'Daniel', 'Ezra', 'Nehemiah', 'Chronicles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase functions are counted in the Hebrew Bible in N and Q clauses and stored in the dictionary func_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elems(level, txt_type):\n",
    "    \n",
    "    elem_count = collections.defaultdict(int)\n",
    "    \n",
    "    for cl in F.otype.s('clause'):\n",
    "        if F.txt.v(cl)[-1] != txt_type:\n",
    "            continue\n",
    "            \n",
    "        words = L.d(cl, 'word')\n",
    "        \n",
    "        # select Hebrew data\n",
    "        if F.language.v(words[0]) != 'Hebrew':\n",
    "            continue\n",
    "    \n",
    "        if level == 'phrase_level':            \n",
    "    \n",
    "            phrases = L.d(cl, 'phrase')\n",
    "        \n",
    "            # a list of phrase functions is collected.\n",
    "            funcs = [F.function.v(ph) for ph in phrases]\n",
    "\n",
    "            for fun in funcs:\n",
    "                elem_count[fun] += 1\n",
    "                \n",
    "        elif level == 'word_level':\n",
    "            \n",
    "            # a list of pos are collected.\n",
    "            poss = [F.sp.v(wo) for wo in words]\n",
    "            \n",
    "            for pos in poss:\n",
    "                elem_count[pos] += 1\n",
    "            \n",
    "    return(elem_count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_language_list(level, cl):\n",
    "    \n",
    "    if level == 'phrase_level':\n",
    "        \n",
    "        phrases = L.d(cl, 'phrase')\n",
    "        elements = [F.function.v(ph) for ph in phrases] \n",
    "        \n",
    "    \n",
    "    elif level == 'word_level':\n",
    "        \n",
    "        words = L.d(cl, 'word')\n",
    "        elements = [F.sp.v(wo) for wo in words]\n",
    "        \n",
    "    return elements\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are collected. The resulting dataset is split in train and validation set.\n",
    "N and Q clauses are collected separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_tf(val_book, level, txt_type):\n",
    "    \"\"\"\n",
    "    The argument of the function is val_book. This is the book on which no data are trained, \n",
    "    but functions as the test set.\n",
    "    \n",
    "    The function returns four objects:\n",
    "    \n",
    "    cl_list_ebh_q, contains the nodes of clauses in EBH books\n",
    "    cl_list_lbh_q, contains the nodes of clauses in LBH books\n",
    "    \n",
    "    targets_dict, assigns the output value to each clause: 0 for EBH clauses, 1 for LBH clauses.\n",
    "    phr_funcs_dict, assigns a list of phrase functions to a clause node\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    cl_list_ebh = []\n",
    "    cl_list_lbh = []\n",
    "    \n",
    "    phr_funcs_dict = {}\n",
    "    targets_dict = {}\n",
    "\n",
    "    for cl in F.otype.s('clause'):\n",
    "        if F.txt.v(cl)[-1] != txt_type:\n",
    "            continue\n",
    "            \n",
    "        words = L.d(cl, 'word')\n",
    "        \n",
    "        # only use Hebrew clauses\n",
    "        if F.language.v(words[0]) != 'Hebrew':\n",
    "            continue\n",
    "        \n",
    "        # do not use val_book\n",
    "        if T.bookName(cl).split('_')[-1] == val_book:\n",
    "            continue\n",
    "        \n",
    "        # process EBH, first remove poetic parts\n",
    "        # 1_Samuel and 2_Samuel are treated as one book, which is achieved in the following line \n",
    "        # (same for Kings and Chronicles)\n",
    "        if T.bookName(cl).split('_')[-1] in ebh:\n",
    "            bo, ch, ve = T.sectionFromNode(cl)\n",
    "            if bo == 'Genesis' and ch == 49 and 1 < ve < 28:\n",
    "                continue\n",
    "            elif bo == 'Exodus' and ch == 15 and ve < 19:\n",
    "                continue\n",
    "            elif bo == 'Numbers' and ch in {23,24}:\n",
    "                continue\n",
    "            elif bo == 'Deuteronomy' and ch in {32,33}:\n",
    "                continue\n",
    "            elif bo == 'Judges' and ch == 5:\n",
    "                continue\n",
    "            elif bo == '1_Samuel' and ch == 2 and ve < 11:\n",
    "                continue\n",
    "            elif bo == '2_Samuel' and ch == 1 and ve > 18:\n",
    "                continue\n",
    "            elif bo == '2_Samuel' and ch == 22:\n",
    "                continue\n",
    "            elif bo == '2_Samuel' and ch == 23 and ve < 8:\n",
    "                continue\n",
    "            \n",
    "            # make list of phrase functions\n",
    "            \n",
    "            lang_list = make_language_list(level, cl)\n",
    "            \n",
    "\n",
    "            phr_funcs_dict[cl] = lang_list\n",
    "            targets_dict[cl] = 0\n",
    "  \n",
    "            cl_list_ebh.append(cl)\n",
    "\n",
    "        # process LBH the same way as EBH\n",
    "        elif T.bookName(cl).split('_')[-1] in lbh:    \n",
    "            bo,ch,ve = T.sectionFromNode(cl)\n",
    "            if bo == 'Daniel' and ch == 2 and 19 < ve < 24:\n",
    "                continue\n",
    "            if bo == 'Daniel' and ch == 8 and 22 < ve < 27:\n",
    "                continue  \n",
    "            if bo == 'Daniel' and ch == 12 and ve < 4:\n",
    "                continue\n",
    "            if bo == 'Nehemiah' and ch == 9 and 5 < ve < 38:\n",
    "                continue    \n",
    "            if bo == '1_Chronicles' and ch == 16 and 7 < ve < 37:\n",
    "                continue\n",
    "            \n",
    "            lang_list = make_language_list(level, cl)\n",
    "\n",
    "            targets_dict[cl] = 1\n",
    "            phr_funcs_dict[cl] = lang_list\n",
    "\n",
    "            cl_list_lbh.append(cl)\n",
    "                \n",
    "    return cl_list_ebh, cl_list_lbh, targets_dict, phr_funcs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data of the test-book are prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_val_book(val_book, level, txt_type):\n",
    "    \"\"\"\n",
    "    The function returns two objects:\n",
    "    \n",
    "    cl_lists, a list containing all nodes of clauses in the test book\n",
    "    funcs_dicts, a dict in which a list of functions is assigned to a clause node\n",
    "    \"\"\"\n",
    "    \n",
    "    cl_list_book = []\n",
    "    book_funcs_dict = {}\n",
    "\n",
    "    # iterate over all the clauses\n",
    "    for cl in F.otype.s('clause'):\n",
    "\n",
    "        if F.txt.v(cl)[-1] != txt_type:\n",
    "             continue\n",
    "                \n",
    "        words = L.d(cl, 'word')\n",
    "        if F.language.v(words[0]) != 'Hebrew':\n",
    "            continue\n",
    "\n",
    "        bo,ch,ve = T.sectionFromNode(cl)\n",
    "        if bo == 'Genesis' and ch == 49 and 1 < ve < 28:\n",
    "            continue\n",
    "        elif bo == 'Exodus' and ch == 15 and ve < 19:\n",
    "            continue\n",
    "        elif bo == 'Numbers' and ch in {23,24}:\n",
    "            continue\n",
    "        elif bo == 'Deuteronomy' and ch in {32,33}:\n",
    "            continue\n",
    "        elif bo == 'Judges' and ch == 5:\n",
    "            continue\n",
    "        elif bo == '1_Samuel' and ch == 2 and ve < 11:\n",
    "            continue\n",
    "        elif bo == '2_Samuel' and ch == 1 and ve > 18:\n",
    "            continue\n",
    "        elif bo == '2_Samuel' and ch == 22:\n",
    "            continue\n",
    "        elif bo == '2_Samuel' and ch == 23 and ve < 8:\n",
    "            continue\n",
    "        if bo == 'Daniel' and ch == 2 and 19 < ve < 24:\n",
    "            continue\n",
    "        if bo == 'Daniel' and ch == 8 and 22 < ve < 27:\n",
    "            continue  \n",
    "        if bo == 'Daniel' and ch == 12 and ve < 4:\n",
    "            continue\n",
    "        if bo == 'Nehemiah' and ch == 9 and 5 < ve < 38:\n",
    "            continue    \n",
    "        if bo == '1_Chronicles' and ch == 16 and 7 < ve < 37:\n",
    "            continue\n",
    "            \n",
    "        # select clauses from the val_book\n",
    "        if bo.split('_')[-1] == val_book:\n",
    "            \n",
    "            lang_list = make_language_list(level, cl)\n",
    "                        \n",
    "            cl_list_book.append(cl)\n",
    "            book_funcs_dict[cl] = lang_list              \n",
    "        \n",
    "    cl_lists = {val_book : cl_list_book}\n",
    "    \n",
    "    funcs_dicts = {val_book:book_funcs_dict}\n",
    "    \n",
    "    return cl_lists, funcs_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare data of the texts of uncertain date: Jonah, the prose tale of Job and Ruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " def prepare_jonah_job_ruth(level, txt_type):\n",
    "    \"\"\"\n",
    "    The function returns two objects:\n",
    "    \n",
    "    cl_lists, a list containing a list of Q clause nodes for each of the three texts\n",
    "    book_funcs_dict, a dict containing three dicts (one for each text) with lists of phrase functions of all Q clauses \n",
    "    \"\"\"\n",
    "    \n",
    "    cl_lists = collections.defaultdict(list)\n",
    "    book_funcs_dict = collections.defaultdict(dict)\n",
    "\n",
    "    for cl in F.otype.s('clause'):\n",
    "        \n",
    "        bo,ch,ve = T.sectionFromNode(cl)\n",
    "        \n",
    "        if F.txt.v(cl)[-1] != txt_type:\n",
    "             continue\n",
    "                \n",
    "        words = L.d(cl, 'word')\n",
    "        if F.language.v(words[0]) != 'Hebrew':\n",
    "            continue\n",
    "\n",
    "        # in the book of Jonah chapter 2 is removed, this is Jonah's Psalm.\n",
    "        if bo == 'Jonah' and ch != 2:\n",
    "            lang_list = make_language_list(level, cl)\n",
    "\n",
    "            cl_lists[bo].append(cl)\n",
    "            book_funcs_dict[bo][cl] = lang_list\n",
    "         \n",
    "        # select Ruth data\n",
    "        if bo == 'Ruth':\n",
    "            lang_list = make_language_list(level, cl)\n",
    "\n",
    "            cl_lists[bo].append(cl)\n",
    "            book_funcs_dict[bo][cl] = lang_list\n",
    "   \n",
    "        # from Job, the prose tale is selected in chapters 1, 2 and 42.\n",
    "        if bo == 'Job' and ch in {1,2}:\n",
    "        \n",
    "            lang_list = make_language_list(level, cl)\n",
    "            \n",
    "            cl_lists[bo].append(cl)\n",
    "            book_funcs_dict[bo][cl] = lang_list\n",
    "            \n",
    "        if bo == 'Job' and ch == 42 and ve > 6:\n",
    "            \n",
    "            lang_list = make_language_list(level, cl)\n",
    "\n",
    "            cl_lists[bo].append(cl)\n",
    "            book_funcs_dict[bo][cl] = lang_list\n",
    "    \n",
    "    return cl_lists, book_funcs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the functions assign_to_ints, make_conv_dict phrase functions are converted to integers, because the network can only process numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_ints(func_count):\n",
    "    \"\"\"\n",
    "    create f2int_dict, which maps phrase functions to integers\n",
    "    \"\"\"\n",
    "\n",
    "    f2int_dict = {}\n",
    "    f_list = []\n",
    "    for value in func_count.values():\n",
    "        f_list.append(value)\n",
    "\n",
    "    sorted_freqs = (sorted(f_list, reverse=True))\n",
    "\n",
    "    for key in func_count.keys():\n",
    "        f2int_dict[key] = sorted_freqs.index(func_count[key]) + 1\n",
    "    \n",
    "    return f2int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv_dict(f2int_dict,list_of_lists):\n",
    "\n",
    "    phr_ints = {}\n",
    "    \n",
    "    for cl_list in list_of_lists:\n",
    "        for clause in cl_list:\n",
    "            func_ints = [f2int_dict[fun] for fun in phr_funcs_dict[clause]]\n",
    "            phr_ints[clause] = func_ints\n",
    "    \n",
    "    return phr_ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In select_clauses clauses are selected randomly for the train set. This is done to make the numer of EBH and LBH clauses equal in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clauses(cl_list_ebh, cl_list_lbh):\n",
    "\n",
    "    idx_ebh = np.random.choice(cl_list_ebh, int(len(cl_list_lbh)), replace = False)\n",
    "    idx_lbh = np.random.choice(cl_list_lbh, int(len(cl_list_lbh)), replace = False)\n",
    "    tot_index = np.concatenate((idx_ebh,idx_lbh), axis = 0)\n",
    "    \n",
    "    return tot_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_integers(tot_index, phr_ints, targets_dict):\n",
    "\n",
    "    selected_input = [phr_ints[cl] for cl in tot_index]\n",
    "    selected_input = np.array(selected_input)\n",
    "\n",
    "    selected_targets = [targets_dict[cl] for cl in tot_index]\n",
    "    selected_targets = np.array(selected_targets)\n",
    "    \n",
    "    return selected_input, selected_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out maximum length of selected N and Q clauses together. Shorter clauses are padded with zeros, because all clauses need to have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_len(selected_input):\n",
    "\n",
    "    max_length = 0\n",
    "    for sub_corp in [selected_input]:\n",
    "        for clause in sub_corp:\n",
    "            if len(clause) > max_length:\n",
    "                max_length = len(clause)\n",
    "        \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are split in train and test set, and sequences are padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(selected_input, selected_targets):\n",
    "\n",
    "    # pad sequences\n",
    "    X_train = sequence.pad_sequences(selected_input, maxlen=max_length)\n",
    "\n",
    "    # train/test split\n",
    "    data_train_cl, data_test_cl, labels_train, labels_test = train_test_split(tot_index, selected_targets, test_size=0.15, random_state=42)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(X_train, selected_targets, test_size=0.15, random_state=42)\n",
    "\n",
    "    return data_train, data_test, labels_train, labels_test, data_test_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In train_LSTM_model the model is trained. \n",
    "\n",
    "The network consists of an embedding layer, two LSTM networks with 300 cells each and dropout to prevent overfitting.\n",
    "Finally there is a dense layer with a single cell, as usual in binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_model(data_train, labels_train, data_test, labels_test, max_length, level, vocab_size):\n",
    "\n",
    "    if level == 'phrase_level':\n",
    "        epochs = 8\n",
    "    elif level == 'word_level':\n",
    "        epochs = 10\n",
    "    \n",
    "    #top_words = 100\n",
    "    embedding_vector_length = 20\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_length))\n",
    "    model.add(LSTM(300, activation = 'relu', return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(300, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=epochs, batch_size=256)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_test, labels_test, model):\n",
    "    \n",
    "    scores = model.evaluate(data_test, labels_test, verbose=0)\n",
    "    y_hat = model.predict_classes(data_test, verbose=0)\n",
    "    \n",
    "    return scores[1]*100, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on val_book and/or book of uncertain date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_book(cl_list, funcs_dict, f2int_dict, model, max_length):\n",
    "    \"\"\"\n",
    "    The function outputs two objects:\n",
    "    \n",
    "    np.sum(y_hat), the total number of clauses classified as LBH (of which each has the value 1)\n",
    "    len(y_hat), total number of predicted clauses in the book.\n",
    "    \"\"\"\n",
    "\n",
    "    phr_ints = []\n",
    "    for clause in cl_list:\n",
    "\n",
    "        func_ints = [f2int_dict[fun] for fun in funcs_dict[clause]]\n",
    "\n",
    "        phr_ints.append(func_ints)\n",
    "\n",
    "    selected_input = np.array(phr_ints)\n",
    "\n",
    "    sel_input = np.array(sequence.pad_sequences(selected_input, maxlen=max_length))\n",
    "    y_hat = model.predict_classes(sel_input, verbose=0)\n",
    "\n",
    "    return np.sum(y_hat), len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genesis phrase_level N\n",
      "Genesis phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 574us/sample - loss: 0.6929 - accuracy: 0.5077 - val_loss: 0.6916 - val_accuracy: 0.5312\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6896 - accuracy: 0.5405 - val_loss: 0.6845 - val_accuracy: 0.5657\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 116us/sample - loss: 0.6798 - accuracy: 0.5766 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6771 - accuracy: 0.5844 - val_loss: 0.6743 - val_accuracy: 0.5845\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6743 - accuracy: 0.5909 - val_loss: 0.6733 - val_accuracy: 0.5728\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 112us/sample - loss: 0.6725 - accuracy: 0.5893 - val_loss: 0.6708 - val_accuracy: 0.5754\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 112us/sample - loss: 0.6734 - accuracy: 0.5894 - val_loss: 0.6714 - val_accuracy: 0.5715\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 112us/sample - loss: 0.6710 - accuracy: 0.5957 - val_loss: 0.6688 - val_accuracy: 0.5793\n",
      "Genesis phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 4s 470us/sample - loss: 0.6930 - accuracy: 0.5172 - val_loss: 0.6920 - val_accuracy: 0.5663\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 103us/sample - loss: 0.6904 - accuracy: 0.5354 - val_loss: 0.6805 - val_accuracy: 0.5787\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 104us/sample - loss: 0.6811 - accuracy: 0.5670 - val_loss: 0.6785 - val_accuracy: 0.5553\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 103us/sample - loss: 0.6800 - accuracy: 0.5784 - val_loss: 0.6683 - val_accuracy: 0.6105\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 102us/sample - loss: 0.6781 - accuracy: 0.5763 - val_loss: 0.6704 - val_accuracy: 0.6066\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 101us/sample - loss: 0.6775 - accuracy: 0.5810 - val_loss: 0.6663 - val_accuracy: 0.6125\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 102us/sample - loss: 0.6756 - accuracy: 0.5848 - val_loss: 0.6703 - val_accuracy: 0.6125\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 102us/sample - loss: 0.6747 - accuracy: 0.5895 - val_loss: 0.6659 - val_accuracy: 0.6138\n",
      "Genesis phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 4s 506us/sample - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6928 - val_accuracy: 0.4909\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6908 - accuracy: 0.5425 - val_loss: 0.6827 - val_accuracy: 0.5592\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6866 - accuracy: 0.5481 - val_loss: 0.6804 - val_accuracy: 0.5748\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6801 - accuracy: 0.5675 - val_loss: 0.6749 - val_accuracy: 0.5806\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6784 - accuracy: 0.5715 - val_loss: 0.6746 - val_accuracy: 0.5897\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 109us/sample - loss: 0.6757 - accuracy: 0.5787 - val_loss: 0.6728 - val_accuracy: 0.5930\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 108us/sample - loss: 0.6745 - accuracy: 0.5789 - val_loss: 0.6737 - val_accuracy: 0.5930\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6756 - accuracy: 0.5823 - val_loss: 0.6737 - val_accuracy: 0.5917\n",
      "Exodus phrase_level N\n",
      "Exodus phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 518us/sample - loss: 0.6930 - accuracy: 0.5111 - val_loss: 0.6926 - val_accuracy: 0.4909\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6895 - accuracy: 0.5359 - val_loss: 0.6833 - val_accuracy: 0.5787\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6791 - accuracy: 0.5761 - val_loss: 0.6752 - val_accuracy: 0.5780\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 132us/sample - loss: 0.6739 - accuracy: 0.5875 - val_loss: 0.6758 - val_accuracy: 0.5819\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6725 - accuracy: 0.5890 - val_loss: 0.6717 - val_accuracy: 0.5806\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 123us/sample - loss: 0.6715 - accuracy: 0.5893 - val_loss: 0.6691 - val_accuracy: 0.5878\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6684 - accuracy: 0.5969 - val_loss: 0.6685 - val_accuracy: 0.5949\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6664 - accuracy: 0.5989 - val_loss: 0.6681 - val_accuracy: 0.6066\n",
      "Exodus phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 607us/sample - loss: 0.6930 - accuracy: 0.4991 - val_loss: 0.6924 - val_accuracy: 0.5371\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6908 - accuracy: 0.5261 - val_loss: 0.6859 - val_accuracy: 0.5579\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 111us/sample - loss: 0.6809 - accuracy: 0.5634 - val_loss: 0.6721 - val_accuracy: 0.5839\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 117us/sample - loss: 0.6761 - accuracy: 0.5798 - val_loss: 0.6651 - val_accuracy: 0.5878\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6746 - accuracy: 0.5836 - val_loss: 0.6640 - val_accuracy: 0.5897\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6733 - accuracy: 0.5878 - val_loss: 0.6623 - val_accuracy: 0.5910\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 117us/sample - loss: 0.6713 - accuracy: 0.5907 - val_loss: 0.6627 - val_accuracy: 0.5936\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6699 - accuracy: 0.5923 - val_loss: 0.6599 - val_accuracy: 0.6027\n",
      "Exodus phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 6s 632us/sample - loss: 0.6929 - accuracy: 0.5024 - val_loss: 0.6922 - val_accuracy: 0.4915\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 127us/sample - loss: 0.6856 - accuracy: 0.5603 - val_loss: 0.6809 - val_accuracy: 0.5689\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 133us/sample - loss: 0.6767 - accuracy: 0.5730 - val_loss: 0.6731 - val_accuracy: 0.5787\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 143us/sample - loss: 0.6726 - accuracy: 0.5809 - val_loss: 0.6713 - val_accuracy: 0.5871\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 140us/sample - loss: 0.6709 - accuracy: 0.5892 - val_loss: 0.6714 - val_accuracy: 0.5923\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6685 - accuracy: 0.5918 - val_loss: 0.6688 - val_accuracy: 0.6001\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 136us/sample - loss: 0.6667 - accuracy: 0.5977 - val_loss: 0.6657 - val_accuracy: 0.6014\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 130us/sample - loss: 0.6671 - accuracy: 0.5992 - val_loss: 0.6643 - val_accuracy: 0.6073\n",
      "Leviticus phrase_level N\n",
      "Leviticus phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 528us/sample - loss: 0.6927 - accuracy: 0.5129 - val_loss: 0.6910 - val_accuracy: 0.5091\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6902 - accuracy: 0.5410 - val_loss: 0.6790 - val_accuracy: 0.5910\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6847 - accuracy: 0.5615 - val_loss: 0.6732 - val_accuracy: 0.6079\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 108us/sample - loss: 0.6785 - accuracy: 0.5725 - val_loss: 0.6672 - val_accuracy: 0.6008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 107us/sample - loss: 0.6756 - accuracy: 0.5864 - val_loss: 0.6690 - val_accuracy: 0.5988\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 107us/sample - loss: 0.6745 - accuracy: 0.5890 - val_loss: 0.6627 - val_accuracy: 0.6131\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 107us/sample - loss: 0.6726 - accuracy: 0.5910 - val_loss: 0.6640 - val_accuracy: 0.6177\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 108us/sample - loss: 0.6721 - accuracy: 0.5907 - val_loss: 0.6624 - val_accuracy: 0.6138\n",
      "Leviticus phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 598us/sample - loss: 0.6930 - accuracy: 0.5098 - val_loss: 0.6931 - val_accuracy: 0.4909\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6898 - accuracy: 0.5351 - val_loss: 0.6828 - val_accuracy: 0.5663\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6769 - accuracy: 0.5824 - val_loss: 0.6868 - val_accuracy: 0.5676\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6811 - accuracy: 0.5668 - val_loss: 0.6774 - val_accuracy: 0.5748\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6711 - accuracy: 0.5934 - val_loss: 0.6756 - val_accuracy: 0.5852\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6697 - accuracy: 0.5973 - val_loss: 0.6758 - val_accuracy: 0.5839\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 128us/sample - loss: 0.6688 - accuracy: 0.5978 - val_loss: 0.6741 - val_accuracy: 0.5910\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6671 - accuracy: 0.6000 - val_loss: 0.6734 - val_accuracy: 0.5917\n",
      "Leviticus phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 608us/sample - loss: 0.6929 - accuracy: 0.5132 - val_loss: 0.6919 - val_accuracy: 0.5351\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6894 - accuracy: 0.5403 - val_loss: 0.6845 - val_accuracy: 0.5455\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6814 - accuracy: 0.5764 - val_loss: 0.6706 - val_accuracy: 0.5878\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6764 - accuracy: 0.5777 - val_loss: 0.6702 - val_accuracy: 0.5943\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6737 - accuracy: 0.5888 - val_loss: 0.6672 - val_accuracy: 0.5962\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6715 - accuracy: 0.5933 - val_loss: 0.6648 - val_accuracy: 0.6021\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6692 - accuracy: 0.5948 - val_loss: 0.6648 - val_accuracy: 0.6073\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6677 - accuracy: 0.5987 - val_loss: 0.6660 - val_accuracy: 0.6040\n",
      "Numbers phrase_level N\n",
      "Numbers phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 572us/sample - loss: 0.6930 - accuracy: 0.5011 - val_loss: 0.6921 - val_accuracy: 0.4909\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 136us/sample - loss: 0.6878 - accuracy: 0.5619 - val_loss: 0.6748 - val_accuracy: 0.5897\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6782 - accuracy: 0.5825 - val_loss: 0.6647 - val_accuracy: 0.5936\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 133us/sample - loss: 0.6751 - accuracy: 0.5883 - val_loss: 0.6608 - val_accuracy: 0.6086\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 137us/sample - loss: 0.6731 - accuracy: 0.5913 - val_loss: 0.6621 - val_accuracy: 0.6073\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 137us/sample - loss: 0.6699 - accuracy: 0.6014 - val_loss: 0.6614 - val_accuracy: 0.6047\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 132us/sample - loss: 0.6723 - accuracy: 0.5933 - val_loss: 0.6563 - val_accuracy: 0.6060\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 132us/sample - loss: 0.6680 - accuracy: 0.6041 - val_loss: 0.6556 - val_accuracy: 0.6086\n",
      "Numbers phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 576us/sample - loss: 0.6929 - accuracy: 0.5126 - val_loss: 0.6917 - val_accuracy: 0.5384\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6886 - accuracy: 0.5390 - val_loss: 0.6755 - val_accuracy: 0.5858\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 123us/sample - loss: 0.6765 - accuracy: 0.5815 - val_loss: 0.6635 - val_accuracy: 0.6177\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 117us/sample - loss: 0.6715 - accuracy: 0.5904 - val_loss: 0.6589 - val_accuracy: 0.6177\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6702 - accuracy: 0.5941 - val_loss: 0.6605 - val_accuracy: 0.6255\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6685 - accuracy: 0.5965 - val_loss: 0.6561 - val_accuracy: 0.6183\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6683 - accuracy: 0.5994 - val_loss: 0.6575 - val_accuracy: 0.6203\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6659 - accuracy: 0.6074 - val_loss: 0.6522 - val_accuracy: 0.6229\n",
      "Numbers phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 547us/sample - loss: 0.6930 - accuracy: 0.5121 - val_loss: 0.6914 - val_accuracy: 0.5839\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6885 - accuracy: 0.5502 - val_loss: 0.6761 - val_accuracy: 0.6183\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6764 - accuracy: 0.5860 - val_loss: 0.6720 - val_accuracy: 0.6125\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6730 - accuracy: 0.5908 - val_loss: 0.6648 - val_accuracy: 0.6138\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 111us/sample - loss: 0.6733 - accuracy: 0.5903 - val_loss: 0.6658 - val_accuracy: 0.6144\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6696 - accuracy: 0.5961 - val_loss: 0.6625 - val_accuracy: 0.6060\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6679 - accuracy: 0.5983 - val_loss: 0.6605 - val_accuracy: 0.6190\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6673 - accuracy: 0.6022 - val_loss: 0.6634 - val_accuracy: 0.6105\n",
      "Deuteronomy phrase_level N\n",
      "Deuteronomy phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 622us/sample - loss: 0.6931 - accuracy: 0.5090 - val_loss: 0.6918 - val_accuracy: 0.5091\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 136us/sample - loss: 0.6897 - accuracy: 0.5362 - val_loss: 0.6790 - val_accuracy: 0.5800\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 134us/sample - loss: 0.6768 - accuracy: 0.5816 - val_loss: 0.6745 - val_accuracy: 0.5956\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 137us/sample - loss: 0.6736 - accuracy: 0.5844 - val_loss: 0.6771 - val_accuracy: 0.5995\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 137us/sample - loss: 0.6728 - accuracy: 0.5908 - val_loss: 0.6749 - val_accuracy: 0.5956\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 152us/sample - loss: 0.6708 - accuracy: 0.5926 - val_loss: 0.6734 - val_accuracy: 0.5969\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 143us/sample - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6737 - val_accuracy: 0.5956\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 143us/sample - loss: 0.6715 - accuracy: 0.5909 - val_loss: 0.6772 - val_accuracy: 0.5936\n",
      "Deuteronomy phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712/8712 [==============================] - 5s 621us/sample - loss: 0.6930 - accuracy: 0.5100 - val_loss: 0.6926 - val_accuracy: 0.5468\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 111us/sample - loss: 0.6895 - accuracy: 0.5329 - val_loss: 0.6935 - val_accuracy: 0.5234\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 112us/sample - loss: 0.6860 - accuracy: 0.5580 - val_loss: 0.6815 - val_accuracy: 0.5670\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 111us/sample - loss: 0.6762 - accuracy: 0.5727 - val_loss: 0.6791 - val_accuracy: 0.5702\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6716 - accuracy: 0.5872 - val_loss: 0.6791 - val_accuracy: 0.5774\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 111us/sample - loss: 0.6700 - accuracy: 0.5851 - val_loss: 0.6766 - val_accuracy: 0.5793\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6682 - accuracy: 0.5918 - val_loss: 0.6798 - val_accuracy: 0.5793\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 113us/sample - loss: 0.6681 - accuracy: 0.5970 - val_loss: 0.6784 - val_accuracy: 0.5813\n",
      "Deuteronomy phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 561us/sample - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6924 - val_accuracy: 0.5104\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6895 - accuracy: 0.5357 - val_loss: 0.6953 - val_accuracy: 0.5163\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6807 - accuracy: 0.5762 - val_loss: 0.6768 - val_accuracy: 0.5793\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6774 - accuracy: 0.5739 - val_loss: 0.6754 - val_accuracy: 0.5813\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 109us/sample - loss: 0.6738 - accuracy: 0.5910 - val_loss: 0.6723 - val_accuracy: 0.5839\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 110us/sample - loss: 0.6715 - accuracy: 0.5924 - val_loss: 0.6709 - val_accuracy: 0.5891\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 113us/sample - loss: 0.6711 - accuracy: 0.5921 - val_loss: 0.6701 - val_accuracy: 0.5904\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6714 - accuracy: 0.5924 - val_loss: 0.6696 - val_accuracy: 0.5904\n",
      "Joshua phrase_level N\n",
      "Joshua phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 608us/sample - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6922 - val_accuracy: 0.5345\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6893 - accuracy: 0.5456 - val_loss: 0.6825 - val_accuracy: 0.5865\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6783 - accuracy: 0.5821 - val_loss: 0.6775 - val_accuracy: 0.5930\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 128us/sample - loss: 0.6703 - accuracy: 0.6018 - val_loss: 0.6747 - val_accuracy: 0.6001\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6682 - accuracy: 0.6071 - val_loss: 0.6739 - val_accuracy: 0.5930\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 123us/sample - loss: 0.6695 - accuracy: 0.6031 - val_loss: 0.6750 - val_accuracy: 0.6053\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6652 - accuracy: 0.6126 - val_loss: 0.6753 - val_accuracy: 0.5975\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 129us/sample - loss: 0.6646 - accuracy: 0.6118 - val_loss: 0.6704 - val_accuracy: 0.5975\n",
      "Joshua phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 572us/sample - loss: 0.6930 - accuracy: 0.5091 - val_loss: 0.6920 - val_accuracy: 0.5793\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 117us/sample - loss: 0.6864 - accuracy: 0.5505 - val_loss: 0.6829 - val_accuracy: 0.5754\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6763 - accuracy: 0.5784 - val_loss: 0.6893 - val_accuracy: 0.5754\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6725 - accuracy: 0.5893 - val_loss: 0.6831 - val_accuracy: 0.5800\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6697 - accuracy: 0.5948 - val_loss: 0.6796 - val_accuracy: 0.5858\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6685 - accuracy: 0.5985 - val_loss: 0.6797 - val_accuracy: 0.5839\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6679 - accuracy: 0.6026 - val_loss: 0.6780 - val_accuracy: 0.5761\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6685 - accuracy: 0.5941 - val_loss: 0.6774 - val_accuracy: 0.5930\n",
      "Joshua phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 612us/sample - loss: 0.6928 - accuracy: 0.5080 - val_loss: 0.6928 - val_accuracy: 0.4909\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6877 - accuracy: 0.5416 - val_loss: 0.6806 - val_accuracy: 0.5702\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6778 - accuracy: 0.5789 - val_loss: 0.6735 - val_accuracy: 0.5956\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6754 - accuracy: 0.5863 - val_loss: 0.6737 - val_accuracy: 0.5923\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6732 - accuracy: 0.5932 - val_loss: 0.6719 - val_accuracy: 0.6027\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6711 - accuracy: 0.5950 - val_loss: 0.6712 - val_accuracy: 0.5982\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6707 - accuracy: 0.6000 - val_loss: 0.6674 - val_accuracy: 0.6040\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 118us/sample - loss: 0.6670 - accuracy: 0.6031 - val_loss: 0.6669 - val_accuracy: 0.5995\n",
      "Judges phrase_level N\n",
      "Judges phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 573us/sample - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6925 - val_accuracy: 0.5936\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 132us/sample - loss: 0.6913 - accuracy: 0.5284 - val_loss: 0.6847 - val_accuracy: 0.5832\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6811 - accuracy: 0.5711 - val_loss: 0.6744 - val_accuracy: 0.5936\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6771 - accuracy: 0.5824 - val_loss: 0.6702 - val_accuracy: 0.6047\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6741 - accuracy: 0.5809 - val_loss: 0.6669 - val_accuracy: 0.6138\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6744 - accuracy: 0.5841 - val_loss: 0.6657 - val_accuracy: 0.6112\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6714 - accuracy: 0.5916 - val_loss: 0.6631 - val_accuracy: 0.6170\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 119us/sample - loss: 0.6701 - accuracy: 0.5941 - val_loss: 0.6692 - val_accuracy: 0.6164\n",
      "Judges phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 610us/sample - loss: 0.6930 - accuracy: 0.5062 - val_loss: 0.6923 - val_accuracy: 0.4909\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 123us/sample - loss: 0.6898 - accuracy: 0.5374 - val_loss: 0.6837 - val_accuracy: 0.5852\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6821 - accuracy: 0.5645 - val_loss: 0.6760 - val_accuracy: 0.5884\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6755 - accuracy: 0.5836 - val_loss: 0.6748 - val_accuracy: 0.5871\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6752 - accuracy: 0.5879 - val_loss: 0.6753 - val_accuracy: 0.5891\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6755 - accuracy: 0.5883 - val_loss: 0.6757 - val_accuracy: 0.5878\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6719 - accuracy: 0.5904 - val_loss: 0.6771 - val_accuracy: 0.5878\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6715 - accuracy: 0.5930 - val_loss: 0.6755 - val_accuracy: 0.5819\n",
      "Judges phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 624us/sample - loss: 0.6928 - accuracy: 0.5098 - val_loss: 0.6926 - val_accuracy: 0.4915\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 117us/sample - loss: 0.6888 - accuracy: 0.5386 - val_loss: 0.6846 - val_accuracy: 0.5507\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6784 - accuracy: 0.5766 - val_loss: 0.6792 - val_accuracy: 0.5709\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 113us/sample - loss: 0.6756 - accuracy: 0.5800 - val_loss: 0.6772 - val_accuracy: 0.5702\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 108us/sample - loss: 0.6741 - accuracy: 0.5902 - val_loss: 0.6790 - val_accuracy: 0.5728\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 114us/sample - loss: 0.6719 - accuracy: 0.5899 - val_loss: 0.6795 - val_accuracy: 0.5787\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 115us/sample - loss: 0.6709 - accuracy: 0.5932 - val_loss: 0.6783 - val_accuracy: 0.5806\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 114us/sample - loss: 0.6693 - accuracy: 0.5964 - val_loss: 0.6760 - val_accuracy: 0.5878\n",
      "Samuel phrase_level N\n",
      "Samuel phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 630us/sample - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6925 - val_accuracy: 0.5137\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6911 - accuracy: 0.5164 - val_loss: 0.6854 - val_accuracy: 0.5715\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 135us/sample - loss: 0.6827 - accuracy: 0.5698 - val_loss: 0.6793 - val_accuracy: 0.5832\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 133us/sample - loss: 0.6776 - accuracy: 0.5756 - val_loss: 0.6760 - val_accuracy: 0.5923\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 139us/sample - loss: 0.6772 - accuracy: 0.5805 - val_loss: 0.6740 - val_accuracy: 0.5975\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6745 - accuracy: 0.5857 - val_loss: 0.6741 - val_accuracy: 0.5956\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6745 - accuracy: 0.5870 - val_loss: 0.6715 - val_accuracy: 0.5969\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6716 - accuracy: 0.5915 - val_loss: 0.6686 - val_accuracy: 0.6021\n",
      "Samuel phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 6s 647us/sample - loss: 0.6931 - accuracy: 0.5107 - val_loss: 0.6925 - val_accuracy: 0.4915\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 133us/sample - loss: 0.6912 - accuracy: 0.5243 - val_loss: 0.6850 - val_accuracy: 0.5670\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 140us/sample - loss: 0.6836 - accuracy: 0.5712 - val_loss: 0.6795 - val_accuracy: 0.5878\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 130us/sample - loss: 0.6820 - accuracy: 0.5763 - val_loss: 0.6803 - val_accuracy: 0.5878\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 136us/sample - loss: 0.6773 - accuracy: 0.5809 - val_loss: 0.6764 - val_accuracy: 0.5949\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 135us/sample - loss: 0.6752 - accuracy: 0.5896 - val_loss: 0.6742 - val_accuracy: 0.5969\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 133us/sample - loss: 0.6744 - accuracy: 0.5923 - val_loss: 0.6731 - val_accuracy: 0.5910\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 134us/sample - loss: 0.6744 - accuracy: 0.5854 - val_loss: 0.6720 - val_accuracy: 0.6027\n",
      "Samuel phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 604us/sample - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6923 - val_accuracy: 0.5111\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 140us/sample - loss: 0.6909 - accuracy: 0.5202 - val_loss: 0.6862 - val_accuracy: 0.5611\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 145us/sample - loss: 0.6839 - accuracy: 0.5584 - val_loss: 0.6787 - val_accuracy: 0.5813\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 157us/sample - loss: 0.6770 - accuracy: 0.5829 - val_loss: 0.6746 - val_accuracy: 0.5930\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 137us/sample - loss: 0.6772 - accuracy: 0.5856 - val_loss: 0.6727 - val_accuracy: 0.5962\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 133us/sample - loss: 0.6748 - accuracy: 0.5861 - val_loss: 0.6717 - val_accuracy: 0.5897\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6726 - accuracy: 0.5926 - val_loss: 0.6687 - val_accuracy: 0.5982\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6710 - accuracy: 0.5924 - val_loss: 0.6676 - val_accuracy: 0.6034\n",
      "Kings phrase_level N\n",
      "Kings phrase_level N 0\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 6s 639us/sample - loss: 0.6928 - accuracy: 0.5140 - val_loss: 0.6909 - val_accuracy: 0.5702\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 129us/sample - loss: 0.6868 - accuracy: 0.5634 - val_loss: 0.6935 - val_accuracy: 0.4954\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 128us/sample - loss: 0.6788 - accuracy: 0.5764 - val_loss: 0.6685 - val_accuracy: 0.5852\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6696 - accuracy: 0.5980 - val_loss: 0.6665 - val_accuracy: 0.5891\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6673 - accuracy: 0.6020 - val_loss: 0.6663 - val_accuracy: 0.5949\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6647 - accuracy: 0.6011 - val_loss: 0.6679 - val_accuracy: 0.5962\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 120us/sample - loss: 0.6681 - accuracy: 0.5984 - val_loss: 0.6655 - val_accuracy: 0.5975\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6623 - accuracy: 0.6090 - val_loss: 0.6663 - val_accuracy: 0.5995\n",
      "Kings phrase_level N 1\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n",
      "8712/8712 [==============================] - 5s 586us/sample - loss: 0.6931 - accuracy: 0.5054 - val_loss: 0.6927 - val_accuracy: 0.5455\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 125us/sample - loss: 0.6896 - accuracy: 0.5442 - val_loss: 0.6875 - val_accuracy: 0.5494\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6790 - accuracy: 0.5808 - val_loss: 0.6792 - val_accuracy: 0.5683\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6783 - accuracy: 0.5846 - val_loss: 0.6740 - val_accuracy: 0.5897\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 127us/sample - loss: 0.6698 - accuracy: 0.5918 - val_loss: 0.6701 - val_accuracy: 0.5995\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6650 - accuracy: 0.5995 - val_loss: 0.6696 - val_accuracy: 0.6014\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 124us/sample - loss: 0.6632 - accuracy: 0.6064 - val_loss: 0.6700 - val_accuracy: 0.6008\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 126us/sample - loss: 0.6614 - accuracy: 0.6117 - val_loss: 0.6682 - val_accuracy: 0.6034\n",
      "Kings phrase_level N 2\n",
      "Train on 8712 samples, validate on 1538 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712/8712 [==============================] - 5s 613us/sample - loss: 0.6929 - accuracy: 0.5139 - val_loss: 0.6918 - val_accuracy: 0.5273\n",
      "Epoch 2/8\n",
      "8712/8712 [==============================] - 1s 131us/sample - loss: 0.6875 - accuracy: 0.5440 - val_loss: 0.6830 - val_accuracy: 0.5689\n",
      "Epoch 3/8\n",
      "8712/8712 [==============================] - 1s 137us/sample - loss: 0.6797 - accuracy: 0.5690 - val_loss: 0.6777 - val_accuracy: 0.5728\n",
      "Epoch 4/8\n",
      "8712/8712 [==============================] - 1s 121us/sample - loss: 0.6750 - accuracy: 0.5772 - val_loss: 0.6738 - val_accuracy: 0.5689\n",
      "Epoch 5/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6692 - accuracy: 0.5871 - val_loss: 0.6712 - val_accuracy: 0.5800\n",
      "Epoch 6/8\n",
      "8712/8712 [==============================] - 1s 123us/sample - loss: 0.6683 - accuracy: 0.5977 - val_loss: 0.6741 - val_accuracy: 0.5845\n",
      "Epoch 7/8\n",
      "8712/8712 [==============================] - 1s 122us/sample - loss: 0.6677 - accuracy: 0.5958 - val_loss: 0.6695 - val_accuracy: 0.5865\n",
      "Epoch 8/8\n",
      "8712/8712 [==============================] - 1s 130us/sample - loss: 0.6679 - accuracy: 0.5992 - val_loss: 0.6687 - val_accuracy: 0.5852\n",
      "Esther phrase_level N\n",
      "Esther phrase_level N 0\n",
      "Train on 7950 samples, validate on 1404 samples\n",
      "Epoch 1/8\n",
      "7950/7950 [==============================] - 5s 627us/sample - loss: 0.6930 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.4850\n",
      "Epoch 2/8\n",
      "7950/7950 [==============================] - 1s 133us/sample - loss: 0.6904 - accuracy: 0.5210 - val_loss: 0.6835 - val_accuracy: 0.5890\n",
      "Epoch 3/8\n",
      "7950/7950 [==============================] - 1s 137us/sample - loss: 0.6789 - accuracy: 0.5701 - val_loss: 0.6869 - val_accuracy: 0.5819\n",
      "Epoch 4/8\n",
      "7950/7950 [==============================] - 1s 139us/sample - loss: 0.6785 - accuracy: 0.5769 - val_loss: 0.6765 - val_accuracy: 0.5947\n",
      "Epoch 5/8\n",
      "7950/7950 [==============================] - 1s 134us/sample - loss: 0.6731 - accuracy: 0.5800 - val_loss: 0.6689 - val_accuracy: 0.6068\n",
      "Epoch 6/8\n",
      "7950/7950 [==============================] - 1s 132us/sample - loss: 0.6704 - accuracy: 0.5874 - val_loss: 0.6694 - val_accuracy: 0.6204\n",
      "Epoch 7/8\n",
      "7950/7950 [==============================] - 1s 133us/sample - loss: 0.6719 - accuracy: 0.5839 - val_loss: 0.6641 - val_accuracy: 0.6154\n",
      "Epoch 8/8\n",
      "7950/7950 [==============================] - 1s 132us/sample - loss: 0.6691 - accuracy: 0.5914 - val_loss: 0.6697 - val_accuracy: 0.6175\n",
      "Esther phrase_level N 1\n",
      "Train on 7950 samples, validate on 1404 samples\n",
      "Epoch 1/8\n",
      "7950/7950 [==============================] - 5s 627us/sample - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6925 - val_accuracy: 0.5641\n",
      "Epoch 2/8\n",
      "7950/7950 [==============================] - 1s 118us/sample - loss: 0.6890 - accuracy: 0.5406 - val_loss: 0.6899 - val_accuracy: 0.5399\n",
      "Epoch 3/8\n",
      "7950/7950 [==============================] - 1s 117us/sample - loss: 0.6778 - accuracy: 0.5774 - val_loss: 0.6793 - val_accuracy: 0.5719\n",
      "Epoch 4/8\n",
      "7950/7950 [==============================] - 1s 122us/sample - loss: 0.6723 - accuracy: 0.5918 - val_loss: 0.6781 - val_accuracy: 0.5712\n",
      "Epoch 5/8\n",
      "7950/7950 [==============================] - 1s 120us/sample - loss: 0.6716 - accuracy: 0.5926 - val_loss: 0.6766 - val_accuracy: 0.5748\n",
      "Epoch 6/8\n",
      "7950/7950 [==============================] - 1s 118us/sample - loss: 0.6730 - accuracy: 0.5911 - val_loss: 0.6758 - val_accuracy: 0.5798\n",
      "Epoch 7/8\n",
      "7950/7950 [==============================] - 1s 118us/sample - loss: 0.6678 - accuracy: 0.5930 - val_loss: 0.6789 - val_accuracy: 0.5705\n",
      "Epoch 8/8\n",
      "7950/7950 [==============================] - 1s 120us/sample - loss: 0.6686 - accuracy: 0.5996 - val_loss: 0.6750 - val_accuracy: 0.5876\n",
      "Esther phrase_level N 2\n",
      "Train on 7950 samples, validate on 1404 samples\n",
      "Epoch 1/8\n",
      "7950/7950 [==============================] - 5s 627us/sample - loss: 0.6931 - accuracy: 0.5122 - val_loss: 0.6929 - val_accuracy: 0.4858\n",
      "Epoch 2/8\n",
      "7950/7950 [==============================] - 1s 123us/sample - loss: 0.6911 - accuracy: 0.5293 - val_loss: 0.6870 - val_accuracy: 0.5677\n",
      "Epoch 3/8\n",
      "7950/7950 [==============================] - 1s 124us/sample - loss: 0.6839 - accuracy: 0.5638 - val_loss: 0.6815 - val_accuracy: 0.5833\n",
      "Epoch 4/8\n",
      "7950/7950 [==============================] - 1s 119us/sample - loss: 0.6779 - accuracy: 0.5781 - val_loss: 0.6715 - val_accuracy: 0.6019\n",
      "Epoch 5/8\n",
      "7950/7950 [==============================] - 1s 119us/sample - loss: 0.6747 - accuracy: 0.5821 - val_loss: 0.6728 - val_accuracy: 0.5976\n",
      "Epoch 6/8\n",
      "7950/7950 [==============================] - 1s 124us/sample - loss: 0.6748 - accuracy: 0.5814 - val_loss: 0.6683 - val_accuracy: 0.6047\n",
      "Epoch 7/8\n",
      "7950/7950 [==============================] - 1s 121us/sample - loss: 0.6705 - accuracy: 0.5909 - val_loss: 0.6659 - val_accuracy: 0.6061\n",
      "Epoch 8/8\n",
      "7950/7950 [==============================] - 1s 126us/sample - loss: 0.6700 - accuracy: 0.5908 - val_loss: 0.6631 - val_accuracy: 0.6132\n",
      "Daniel phrase_level N\n",
      "Daniel phrase_level N 0\n",
      "Train on 8304 samples, validate on 1466 samples\n",
      "Epoch 1/8\n",
      "8304/8304 [==============================] - 5s 626us/sample - loss: 0.6929 - accuracy: 0.5048 - val_loss: 0.6918 - val_accuracy: 0.5505\n",
      "Epoch 2/8\n",
      "8304/8304 [==============================] - 1s 120us/sample - loss: 0.6856 - accuracy: 0.5649 - val_loss: 0.6833 - val_accuracy: 0.5409\n",
      "Epoch 3/8\n",
      "8304/8304 [==============================] - 1s 119us/sample - loss: 0.6768 - accuracy: 0.5813 - val_loss: 0.6752 - val_accuracy: 0.5737\n",
      "Epoch 4/8\n",
      "8304/8304 [==============================] - 1s 118us/sample - loss: 0.6725 - accuracy: 0.5896 - val_loss: 0.6752 - val_accuracy: 0.5778\n",
      "Epoch 5/8\n",
      "8304/8304 [==============================] - 1s 118us/sample - loss: 0.6725 - accuracy: 0.5955 - val_loss: 0.6764 - val_accuracy: 0.5778\n",
      "Epoch 6/8\n",
      "8304/8304 [==============================] - 1s 117us/sample - loss: 0.6722 - accuracy: 0.5889 - val_loss: 0.6715 - val_accuracy: 0.5975\n",
      "Epoch 7/8\n",
      "8304/8304 [==============================] - 1s 120us/sample - loss: 0.6694 - accuracy: 0.5949 - val_loss: 0.6717 - val_accuracy: 0.5948\n",
      "Epoch 8/8\n",
      "8304/8304 [==============================] - 1s 119us/sample - loss: 0.6670 - accuracy: 0.6068 - val_loss: 0.6681 - val_accuracy: 0.5969\n",
      "Daniel phrase_level N 1\n",
      "Train on 8304 samples, validate on 1466 samples\n",
      "Epoch 1/8\n",
      "8304/8304 [==============================] - 5s 629us/sample - loss: 0.6930 - accuracy: 0.5065 - val_loss: 0.6925 - val_accuracy: 0.5566\n",
      "Epoch 2/8\n",
      "8304/8304 [==============================] - 1s 132us/sample - loss: 0.6892 - accuracy: 0.5436 - val_loss: 0.6851 - val_accuracy: 0.5723\n",
      "Epoch 3/8\n",
      "8304/8304 [==============================] - 1s 129us/sample - loss: 0.6797 - accuracy: 0.5772 - val_loss: 0.6785 - val_accuracy: 0.5744\n",
      "Epoch 4/8\n",
      "8304/8304 [==============================] - 1s 127us/sample - loss: 0.6750 - accuracy: 0.5829 - val_loss: 0.6796 - val_accuracy: 0.5825\n",
      "Epoch 5/8\n",
      "8304/8304 [==============================] - 1s 128us/sample - loss: 0.6737 - accuracy: 0.5920 - val_loss: 0.6744 - val_accuracy: 0.5846\n",
      "Epoch 6/8\n",
      "8304/8304 [==============================] - 1s 129us/sample - loss: 0.6691 - accuracy: 0.5907 - val_loss: 0.6745 - val_accuracy: 0.5784\n",
      "Epoch 7/8\n",
      "8304/8304 [==============================] - 1s 130us/sample - loss: 0.6704 - accuracy: 0.5942 - val_loss: 0.6728 - val_accuracy: 0.5900\n",
      "Epoch 8/8\n",
      "8304/8304 [==============================] - 1s 122us/sample - loss: 0.6658 - accuracy: 0.6009 - val_loss: 0.6713 - val_accuracy: 0.5853\n",
      "Daniel phrase_level N 2\n",
      "Train on 8304 samples, validate on 1466 samples\n",
      "Epoch 1/8\n",
      "8304/8304 [==============================] - 5s 637us/sample - loss: 0.6928 - accuracy: 0.5060 - val_loss: 0.6921 - val_accuracy: 0.4986\n",
      "Epoch 2/8\n",
      "8304/8304 [==============================] - 1s 129us/sample - loss: 0.6873 - accuracy: 0.5474 - val_loss: 0.6802 - val_accuracy: 0.5737\n",
      "Epoch 3/8\n",
      "8304/8304 [==============================] - 1s 131us/sample - loss: 0.6752 - accuracy: 0.5848 - val_loss: 0.6681 - val_accuracy: 0.6078\n",
      "Epoch 4/8\n",
      "8304/8304 [==============================] - 1s 132us/sample - loss: 0.6731 - accuracy: 0.5927 - val_loss: 0.6647 - val_accuracy: 0.6064\n",
      "Epoch 5/8\n",
      "8304/8304 [==============================] - 1s 129us/sample - loss: 0.6702 - accuracy: 0.5961 - val_loss: 0.6614 - val_accuracy: 0.6146\n",
      "Epoch 6/8\n",
      "8304/8304 [==============================] - 1s 126us/sample - loss: 0.6684 - accuracy: 0.6007 - val_loss: 0.6672 - val_accuracy: 0.5969\n",
      "Epoch 7/8\n",
      "8304/8304 [==============================] - 1s 127us/sample - loss: 0.6691 - accuracy: 0.6021 - val_loss: 0.6572 - val_accuracy: 0.6201\n",
      "Epoch 8/8\n",
      "8304/8304 [==============================] - 1s 136us/sample - loss: 0.6662 - accuracy: 0.6039 - val_loss: 0.6573 - val_accuracy: 0.6194\n",
      "Ezra phrase_level N\n",
      "Ezra phrase_level N 0\n",
      "Train on 8115 samples, validate on 1433 samples\n",
      "Epoch 1/8\n",
      "8115/8115 [==============================] - 5s 616us/sample - loss: 0.6930 - accuracy: 0.5034 - val_loss: 0.6929 - val_accuracy: 0.4857\n",
      "Epoch 2/8\n",
      "8115/8115 [==============================] - 1s 114us/sample - loss: 0.6894 - accuracy: 0.5411 - val_loss: 0.6859 - val_accuracy: 0.5534\n",
      "Epoch 3/8\n",
      "8115/8115 [==============================] - 1s 118us/sample - loss: 0.6783 - accuracy: 0.5744 - val_loss: 0.6883 - val_accuracy: 0.5597\n",
      "Epoch 4/8\n",
      "8115/8115 [==============================] - 1s 126us/sample - loss: 0.6756 - accuracy: 0.5772 - val_loss: 0.6845 - val_accuracy: 0.5666\n",
      "Epoch 5/8\n",
      "8115/8115 [==============================] - 1s 118us/sample - loss: 0.6720 - accuracy: 0.5872 - val_loss: 0.6842 - val_accuracy: 0.5722\n",
      "Epoch 6/8\n",
      "8115/8115 [==============================] - 1s 127us/sample - loss: 0.6690 - accuracy: 0.5952 - val_loss: 0.6861 - val_accuracy: 0.5764\n",
      "Epoch 7/8\n",
      "8115/8115 [==============================] - 1s 139us/sample - loss: 0.6722 - accuracy: 0.5876 - val_loss: 0.6866 - val_accuracy: 0.5729\n",
      "Epoch 8/8\n",
      "8115/8115 [==============================] - 1s 127us/sample - loss: 0.6680 - accuracy: 0.6004 - val_loss: 0.6770 - val_accuracy: 0.5897\n",
      "Ezra phrase_level N 1\n",
      "Train on 8115 samples, validate on 1433 samples\n",
      "Epoch 1/8\n",
      "8115/8115 [==============================] - 6s 726us/sample - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.6935 - val_accuracy: 0.4857\n",
      "Epoch 2/8\n",
      "8115/8115 [==============================] - 1s 128us/sample - loss: 0.6912 - accuracy: 0.5205 - val_loss: 0.6871 - val_accuracy: 0.5715\n",
      "Epoch 3/8\n",
      "8115/8115 [==============================] - 1s 130us/sample - loss: 0.6834 - accuracy: 0.5645 - val_loss: 0.6777 - val_accuracy: 0.5813\n",
      "Epoch 4/8\n",
      "8115/8115 [==============================] - 1s 119us/sample - loss: 0.6788 - accuracy: 0.5719 - val_loss: 0.6774 - val_accuracy: 0.5792\n",
      "Epoch 5/8\n",
      "8115/8115 [==============================] - 1s 130us/sample - loss: 0.6765 - accuracy: 0.5783 - val_loss: 0.6740 - val_accuracy: 0.5764\n",
      "Epoch 6/8\n",
      "8115/8115 [==============================] - 1s 125us/sample - loss: 0.6747 - accuracy: 0.5855 - val_loss: 0.6747 - val_accuracy: 0.5792\n",
      "Epoch 7/8\n",
      "8115/8115 [==============================] - 1s 122us/sample - loss: 0.6737 - accuracy: 0.5846 - val_loss: 0.6725 - val_accuracy: 0.5911\n",
      "Epoch 8/8\n",
      "8115/8115 [==============================] - 1s 119us/sample - loss: 0.6729 - accuracy: 0.5877 - val_loss: 0.6710 - val_accuracy: 0.5953\n",
      "Ezra phrase_level N 2\n",
      "Train on 8115 samples, validate on 1433 samples\n",
      "Epoch 1/8\n",
      "8115/8115 [==============================] - 6s 728us/sample - loss: 0.6930 - accuracy: 0.5030 - val_loss: 0.6926 - val_accuracy: 0.5262\n",
      "Epoch 2/8\n",
      "8115/8115 [==============================] - 1s 145us/sample - loss: 0.6897 - accuracy: 0.5375 - val_loss: 0.6841 - val_accuracy: 0.5652\n",
      "Epoch 3/8\n",
      "8115/8115 [==============================] - 1s 145us/sample - loss: 0.6781 - accuracy: 0.5708 - val_loss: 0.6811 - val_accuracy: 0.5729\n",
      "Epoch 4/8\n",
      "8115/8115 [==============================] - 1s 151us/sample - loss: 0.6742 - accuracy: 0.5800 - val_loss: 0.6770 - val_accuracy: 0.5869\n",
      "Epoch 5/8\n",
      "8115/8115 [==============================] - 1s 136us/sample - loss: 0.6717 - accuracy: 0.5931 - val_loss: 0.6792 - val_accuracy: 0.5932\n",
      "Epoch 6/8\n",
      "8115/8115 [==============================] - 1s 123us/sample - loss: 0.6724 - accuracy: 0.5917 - val_loss: 0.6758 - val_accuracy: 0.5897\n",
      "Epoch 7/8\n",
      "8115/8115 [==============================] - 1s 117us/sample - loss: 0.6700 - accuracy: 0.5904 - val_loss: 0.6828 - val_accuracy: 0.5736\n",
      "Epoch 8/8\n",
      "8115/8115 [==============================] - 1s 116us/sample - loss: 0.6702 - accuracy: 0.5962 - val_loss: 0.6747 - val_accuracy: 0.5946\n",
      "Nehemiah phrase_level N\n",
      "Nehemiah phrase_level N 0\n",
      "Train on 7667 samples, validate on 1353 samples\n",
      "Epoch 1/8\n",
      "7667/7667 [==============================] - 5s 659us/sample - loss: 0.6932 - accuracy: 0.4971 - val_loss: 0.6930 - val_accuracy: 0.4871\n",
      "Epoch 2/8\n",
      "7667/7667 [==============================] - 1s 132us/sample - loss: 0.6905 - accuracy: 0.5442 - val_loss: 0.6834 - val_accuracy: 0.5905\n",
      "Epoch 3/8\n",
      "7667/7667 [==============================] - 1s 130us/sample - loss: 0.6797 - accuracy: 0.5732 - val_loss: 0.6728 - val_accuracy: 0.5876\n",
      "Epoch 4/8\n",
      "7667/7667 [==============================] - 1s 135us/sample - loss: 0.6748 - accuracy: 0.5852 - val_loss: 0.6667 - val_accuracy: 0.5913\n",
      "Epoch 5/8\n",
      "7667/7667 [==============================] - 1s 134us/sample - loss: 0.6720 - accuracy: 0.5881 - val_loss: 0.6637 - val_accuracy: 0.6009\n",
      "Epoch 6/8\n",
      "7667/7667 [==============================] - 1s 132us/sample - loss: 0.6696 - accuracy: 0.5933 - val_loss: 0.6608 - val_accuracy: 0.5972\n",
      "Epoch 7/8\n",
      "7667/7667 [==============================] - 1s 134us/sample - loss: 0.6670 - accuracy: 0.5992 - val_loss: 0.6601 - val_accuracy: 0.6009\n",
      "Epoch 8/8\n",
      "7667/7667 [==============================] - 1s 130us/sample - loss: 0.6652 - accuracy: 0.5995 - val_loss: 0.6601 - val_accuracy: 0.6061\n",
      "Nehemiah phrase_level N 1\n",
      "Train on 7667 samples, validate on 1353 samples\n",
      "Epoch 1/8\n",
      "7667/7667 [==============================] - 6s 720us/sample - loss: 0.6930 - accuracy: 0.5053 - val_loss: 0.6925 - val_accuracy: 0.5388\n",
      "Epoch 2/8\n",
      "7667/7667 [==============================] - 1s 130us/sample - loss: 0.6895 - accuracy: 0.5457 - val_loss: 0.6848 - val_accuracy: 0.5469\n",
      "Epoch 3/8\n",
      "7667/7667 [==============================] - 1s 122us/sample - loss: 0.6809 - accuracy: 0.5698 - val_loss: 0.6809 - val_accuracy: 0.5765\n",
      "Epoch 4/8\n",
      "7667/7667 [==============================] - 1s 122us/sample - loss: 0.6777 - accuracy: 0.5751 - val_loss: 0.6794 - val_accuracy: 0.5691\n",
      "Epoch 5/8\n",
      "7667/7667 [==============================] - 1s 120us/sample - loss: 0.6741 - accuracy: 0.5822 - val_loss: 0.6811 - val_accuracy: 0.5706\n",
      "Epoch 6/8\n",
      "7667/7667 [==============================] - 1s 125us/sample - loss: 0.6759 - accuracy: 0.5821 - val_loss: 0.6782 - val_accuracy: 0.5735\n",
      "Epoch 7/8\n",
      "7667/7667 [==============================] - 1s 126us/sample - loss: 0.6744 - accuracy: 0.5808 - val_loss: 0.6765 - val_accuracy: 0.5758\n",
      "Epoch 8/8\n",
      "7667/7667 [==============================] - 1s 128us/sample - loss: 0.6719 - accuracy: 0.5852 - val_loss: 0.6747 - val_accuracy: 0.5780\n",
      "Nehemiah phrase_level N 2\n",
      "Train on 7667 samples, validate on 1353 samples\n",
      "Epoch 1/8\n",
      "7667/7667 [==============================] - 5s 699us/sample - loss: 0.6931 - accuracy: 0.5093 - val_loss: 0.6926 - val_accuracy: 0.5129\n",
      "Epoch 2/8\n",
      "7667/7667 [==============================] - 1s 128us/sample - loss: 0.6914 - accuracy: 0.5299 - val_loss: 0.6858 - val_accuracy: 0.5787\n",
      "Epoch 3/8\n",
      "7667/7667 [==============================] - 1s 124us/sample - loss: 0.6820 - accuracy: 0.5706 - val_loss: 0.6706 - val_accuracy: 0.6009\n",
      "Epoch 4/8\n",
      "7667/7667 [==============================] - 1s 125us/sample - loss: 0.6766 - accuracy: 0.5815 - val_loss: 0.6738 - val_accuracy: 0.5979\n",
      "Epoch 5/8\n",
      "7667/7667 [==============================] - 1s 126us/sample - loss: 0.6740 - accuracy: 0.5907 - val_loss: 0.6665 - val_accuracy: 0.6009\n",
      "Epoch 6/8\n",
      "7667/7667 [==============================] - 1s 125us/sample - loss: 0.6733 - accuracy: 0.5902 - val_loss: 0.6667 - val_accuracy: 0.6083\n",
      "Epoch 7/8\n",
      "7667/7667 [==============================] - 1s 125us/sample - loss: 0.6709 - accuracy: 0.5928 - val_loss: 0.6645 - val_accuracy: 0.6120\n",
      "Epoch 8/8\n",
      "7667/7667 [==============================] - 1s 148us/sample - loss: 0.6692 - accuracy: 0.5935 - val_loss: 0.6651 - val_accuracy: 0.6120\n",
      "Chronicles phrase_level N\n",
      "Chronicles phrase_level N 0\n",
      "Train on 2811 samples, validate on 497 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811/2811 [==============================] - 5s 2ms/sample - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6929 - val_accuracy: 0.5674\n",
      "Epoch 2/8\n",
      "2811/2811 [==============================] - 0s 119us/sample - loss: 0.6927 - accuracy: 0.5318 - val_loss: 0.6924 - val_accuracy: 0.5493\n",
      "Epoch 3/8\n",
      "2811/2811 [==============================] - 0s 125us/sample - loss: 0.6916 - accuracy: 0.5414 - val_loss: 0.6901 - val_accuracy: 0.5433\n",
      "Epoch 4/8\n",
      "2811/2811 [==============================] - 0s 120us/sample - loss: 0.6876 - accuracy: 0.5617 - val_loss: 0.6836 - val_accuracy: 0.5553\n",
      "Epoch 5/8\n",
      "2811/2811 [==============================] - 0s 125us/sample - loss: 0.6820 - accuracy: 0.5592 - val_loss: 0.6846 - val_accuracy: 0.5453\n",
      "Epoch 6/8\n",
      "2811/2811 [==============================] - 0s 125us/sample - loss: 0.6776 - accuracy: 0.5788 - val_loss: 0.6728 - val_accuracy: 0.6016\n",
      "Epoch 7/8\n",
      "2811/2811 [==============================] - 0s 127us/sample - loss: 0.6718 - accuracy: 0.5863 - val_loss: 0.6664 - val_accuracy: 0.6076\n",
      "Epoch 8/8\n",
      "2811/2811 [==============================] - 0s 124us/sample - loss: 0.6730 - accuracy: 0.5898 - val_loss: 0.6663 - val_accuracy: 0.6117\n",
      "Chronicles phrase_level N 1\n",
      "Train on 2811 samples, validate on 497 samples\n",
      "Epoch 1/8\n",
      "2811/2811 [==============================] - 5s 2ms/sample - loss: 0.6932 - accuracy: 0.4948 - val_loss: 0.6932 - val_accuracy: 0.4829\n",
      "Epoch 2/8\n",
      "2811/2811 [==============================] - 0s 107us/sample - loss: 0.6931 - accuracy: 0.5062 - val_loss: 0.6931 - val_accuracy: 0.4829\n",
      "Epoch 3/8\n",
      "2811/2811 [==============================] - 0s 117us/sample - loss: 0.6928 - accuracy: 0.5162 - val_loss: 0.6926 - val_accuracy: 0.4829\n",
      "Epoch 4/8\n",
      "2811/2811 [==============================] - 0s 137us/sample - loss: 0.6921 - accuracy: 0.5240 - val_loss: 0.6912 - val_accuracy: 0.5231\n",
      "Epoch 5/8\n",
      "2811/2811 [==============================] - 0s 128us/sample - loss: 0.6899 - accuracy: 0.5592 - val_loss: 0.6853 - val_accuracy: 0.5956\n",
      "Epoch 6/8\n",
      "2811/2811 [==============================] - 0s 114us/sample - loss: 0.6847 - accuracy: 0.5582 - val_loss: 0.6811 - val_accuracy: 0.5533\n",
      "Epoch 7/8\n",
      "2811/2811 [==============================] - 0s 110us/sample - loss: 0.6813 - accuracy: 0.5660 - val_loss: 0.6746 - val_accuracy: 0.6197\n",
      "Epoch 8/8\n",
      "2811/2811 [==============================] - 0s 121us/sample - loss: 0.6731 - accuracy: 0.5859 - val_loss: 0.6710 - val_accuracy: 0.6016\n",
      "Chronicles phrase_level N 2\n",
      "Train on 2811 samples, validate on 497 samples\n",
      "Epoch 1/8\n",
      "2811/2811 [==============================] - 5s 2ms/sample - loss: 0.6931 - accuracy: 0.4966 - val_loss: 0.6933 - val_accuracy: 0.4829\n",
      "Epoch 2/8\n",
      "2811/2811 [==============================] - 0s 110us/sample - loss: 0.6930 - accuracy: 0.5180 - val_loss: 0.6929 - val_accuracy: 0.5191\n",
      "Epoch 3/8\n",
      "2811/2811 [==============================] - 0s 116us/sample - loss: 0.6922 - accuracy: 0.5208 - val_loss: 0.6925 - val_accuracy: 0.5030\n",
      "Epoch 4/8\n",
      "2811/2811 [==============================] - 0s 114us/sample - loss: 0.6899 - accuracy: 0.5439 - val_loss: 0.6895 - val_accuracy: 0.5372\n",
      "Epoch 5/8\n",
      "2811/2811 [==============================] - 0s 113us/sample - loss: 0.6846 - accuracy: 0.5671 - val_loss: 0.6880 - val_accuracy: 0.5392\n",
      "Epoch 6/8\n",
      "2811/2811 [==============================] - 0s 115us/sample - loss: 0.6802 - accuracy: 0.5756 - val_loss: 0.6868 - val_accuracy: 0.5553\n",
      "Epoch 7/8\n",
      "2811/2811 [==============================] - 0s 139us/sample - loss: 0.6774 - accuracy: 0.5891 - val_loss: 0.6843 - val_accuracy: 0.5755\n",
      "Epoch 8/8\n",
      "2811/2811 [==============================] - 0s 151us/sample - loss: 0.6747 - accuracy: 0.5859 - val_loss: 0.6823 - val_accuracy: 0.5714\n"
     ]
    }
   ],
   "source": [
    "all_accuracy_dict = {}\n",
    "\n",
    "validation_preds = collections.defaultdict(list)\n",
    "jo_jo_ru_preds = collections.defaultdict(list)\n",
    "clause_counts = collections.defaultdict(list)\n",
    "\n",
    "cl_lists, funcs_dicts = prepare_jonah_job_ruth(level, txt_type)\n",
    "\n",
    "# iterate over all test books\n",
    "for validation_book in ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', 'Samuel', 'Kings', 'Esther', 'Daniel', 'Ezra', 'Nehemiah', 'Chronicles']:\n",
    "    print(validation_book, level, txt_type)\n",
    "    \n",
    "    func_count = count_elems(level, txt_type)\n",
    "    cl_list_ebh, cl_list_lbh, targets_dict, phr_funcs_dict = get_data_from_tf(validation_book, level, txt_type)\n",
    "    cl_dicts_val, funcs_dicts_val = prepare_val_book(validation_book, level, txt_type)\n",
    "\n",
    "    f2int_dict = assign_to_ints(func_count)\n",
    "    list_of_lists = [cl_list_ebh, cl_list_lbh]\n",
    "    phr_ints = make_conv_dict(f2int_dict,list_of_lists)\n",
    "\n",
    "    verse_dict = collections.defaultdict(list)\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    # Train the model and make predictions, 200 times for each EBH/LBH book\n",
    "    for i in range(200):\n",
    "        print(validation_book, level, txt_type, i)\n",
    "        \n",
    "        tot_index = select_clauses(cl_list_ebh, cl_list_lbh)\n",
    "        selected_input, selected_targets = convert_to_integers(tot_index, phr_ints, targets_dict)\n",
    "        max_length = calc_max_len(selected_input)\n",
    "        data_train, data_test, labels_train, labels_test, data_test_cl = test_train_split(selected_input, selected_targets)\n",
    "\n",
    "        model = train_LSTM_model(data_train, labels_train, data_test, labels_test, max_length, level, len(func_count))\n",
    "        accuracy, y_hat = evaluate_model(data_test, labels_test, model)\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        sum_pred, tot_clauses = predict_book(cl_dicts_val[validation_book], funcs_dicts_val[validation_book], f2int_dict, model, max_length)\n",
    "        \n",
    "        #validation \n",
    "        validation_preds[validation_book].append(sum_pred)\n",
    "        clause_counts[validation_book].append(tot_clauses)\n",
    "        \n",
    "        for uncertain_book in {\"Jonah\", \"Ruth\", \"Job\"}:\n",
    "            sum_pred, tot_clauses = predict_book(cl_lists[uncertain_book], funcs_dicts[uncertain_book], f2int_dict, model, max_length)\n",
    "            jo_jo_ru_preds[uncertain_book].append(sum_pred)\n",
    "            clause_counts[uncertain_book].append(tot_clauses)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "    all_accuracy_dict[validation_book] = accuracy_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all relevant objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jo_jo_ru_df = pd.DataFrame.from_dict(jo_jo_ru_preds)\n",
    "filename = 'jo_jo_ru_preds_' + level + '_' + txt_type + '.csv'\n",
    "jo_jo_ru_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job</th>\n",
       "      <th>Jonah</th>\n",
       "      <th>Ruth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Job  Jonah  Ruth\n",
       "0   36     25    33\n",
       "1   48     30    43\n",
       "2   45     26    36\n",
       "3   49     30    41\n",
       "4   38     28    33"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jo_jo_ru_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebh_lbh_lengths = {}\n",
    "jo_jo_ru_length = {}\n",
    "\n",
    "books = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', 'Samuel', 'Kings', 'Esther', 'Daniel', 'Ezra', 'Nehemiah', 'Chronicles']\n",
    "         \n",
    "for bo in books:\n",
    "    ebh_lbh_lengths[bo] = clause_counts[bo]\n",
    "         \n",
    "for bo in ['Jonah', 'Job', 'Ruth']:\n",
    "         jo_jo_ru_length[bo] = clause_counts[bo]\n",
    "         \n",
    "jo_jo_ru_len_df = pd.DataFrame.from_dict(jo_jo_ru_length)\n",
    "\n",
    "\n",
    "ebh_lbh_lengths_df = pd.DataFrame.from_dict(ebh_lbh_lengths)\n",
    "\n",
    "all_lengths = pd.concat([jo_jo_ru_len_df.head(1), ebh_lbh_lengths_df.head(1)], axis=1)\n",
    "\n",
    "filename = 'clause_counts_' + level + '_' + txt_type + '.csv'\n",
    "all_lengths.to_csv(filename, index=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jonah</th>\n",
       "      <th>Job</th>\n",
       "      <th>Ruth</th>\n",
       "      <th>Genesis</th>\n",
       "      <th>Exodus</th>\n",
       "      <th>Leviticus</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Deuteronomy</th>\n",
       "      <th>Joshua</th>\n",
       "      <th>Judges</th>\n",
       "      <th>Samuel</th>\n",
       "      <th>Kings</th>\n",
       "      <th>Esther</th>\n",
       "      <th>Daniel</th>\n",
       "      <th>Ezra</th>\n",
       "      <th>Nehemiah</th>\n",
       "      <th>Chronicles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>141</td>\n",
       "      <td>201</td>\n",
       "      <td>3300</td>\n",
       "      <td>1633</td>\n",
       "      <td>296</td>\n",
       "      <td>1500</td>\n",
       "      <td>460</td>\n",
       "      <td>1420</td>\n",
       "      <td>1756</td>\n",
       "      <td>3809</td>\n",
       "      <td>4038</td>\n",
       "      <td>448</td>\n",
       "      <td>240</td>\n",
       "      <td>351</td>\n",
       "      <td>615</td>\n",
       "      <td>3471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Jonah  Job  Ruth  Genesis  Exodus  Leviticus  Numbers  Deuteronomy  Joshua  \\\n",
       "0    110  141   201     3300    1633        296     1500          460    1420   \n",
       "\n",
       "   Judges  Samuel  Kings  Esther  Daniel  Ezra  Nehemiah  Chronicles  \n",
       "0    1756    3809   4038     448     240   351       615        3471  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are saved in csv files, for instance, [this one](validation_preds_phrase_level_N.csv) for N clauses with phrase level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_preds_df = pd.DataFrame.from_dict(validation_preds)\n",
    "filename = 'validation_preds_' + level + '_' + txt_type + '.csv'\n",
    "validation_preds_df.to_csv(filename, index=False)\n",
    "\n",
    "all_accuracy_df = pd.DataFrame.from_dict(all_accuracy_dict)\n",
    "filename = 'all_accuracy_' + level + '_' + txt_type + '.csv'\n",
    "all_accuracy_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genesis</th>\n",
       "      <th>Exodus</th>\n",
       "      <th>Leviticus</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Deuteronomy</th>\n",
       "      <th>Joshua</th>\n",
       "      <th>Judges</th>\n",
       "      <th>Samuel</th>\n",
       "      <th>Kings</th>\n",
       "      <th>Esther</th>\n",
       "      <th>Daniel</th>\n",
       "      <th>Ezra</th>\n",
       "      <th>Nehemiah</th>\n",
       "      <th>Chronicles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>891</td>\n",
       "      <td>665</td>\n",
       "      <td>72</td>\n",
       "      <td>681</td>\n",
       "      <td>225</td>\n",
       "      <td>629</td>\n",
       "      <td>774</td>\n",
       "      <td>1131</td>\n",
       "      <td>1631</td>\n",
       "      <td>250</td>\n",
       "      <td>124</td>\n",
       "      <td>236</td>\n",
       "      <td>402</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1107</td>\n",
       "      <td>659</td>\n",
       "      <td>87</td>\n",
       "      <td>640</td>\n",
       "      <td>247</td>\n",
       "      <td>587</td>\n",
       "      <td>478</td>\n",
       "      <td>1149</td>\n",
       "      <td>1478</td>\n",
       "      <td>220</td>\n",
       "      <td>106</td>\n",
       "      <td>235</td>\n",
       "      <td>299</td>\n",
       "      <td>2299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>978</td>\n",
       "      <td>557</td>\n",
       "      <td>79</td>\n",
       "      <td>587</td>\n",
       "      <td>200</td>\n",
       "      <td>676</td>\n",
       "      <td>512</td>\n",
       "      <td>1184</td>\n",
       "      <td>1399</td>\n",
       "      <td>195</td>\n",
       "      <td>133</td>\n",
       "      <td>238</td>\n",
       "      <td>305</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genesis  Exodus  Leviticus  Numbers  Deuteronomy  Joshua  Judges  Samuel  \\\n",
       "0      891     665         72      681          225     629     774    1131   \n",
       "1     1107     659         87      640          247     587     478    1149   \n",
       "2      978     557         79      587          200     676     512    1184   \n",
       "\n",
       "   Kings  Esther  Daniel  Ezra  Nehemiah  Chronicles  \n",
       "0   1631     250     124   236       402        2023  \n",
       "1   1478     220     106   235       299        2299  \n",
       "2   1399     195     133   238       305        1521  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_preds_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genesis</th>\n",
       "      <th>Exodus</th>\n",
       "      <th>Leviticus</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Deuteronomy</th>\n",
       "      <th>Joshua</th>\n",
       "      <th>Judges</th>\n",
       "      <th>Samuel</th>\n",
       "      <th>Kings</th>\n",
       "      <th>Esther</th>\n",
       "      <th>Daniel</th>\n",
       "      <th>Ezra</th>\n",
       "      <th>Nehemiah</th>\n",
       "      <th>Chronicles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.932377</td>\n",
       "      <td>60.663199</td>\n",
       "      <td>61.378413</td>\n",
       "      <td>60.858256</td>\n",
       "      <td>59.362811</td>\n",
       "      <td>59.752923</td>\n",
       "      <td>61.638492</td>\n",
       "      <td>60.208064</td>\n",
       "      <td>59.947985</td>\n",
       "      <td>61.752135</td>\n",
       "      <td>59.686220</td>\n",
       "      <td>58.967203</td>\n",
       "      <td>60.606062</td>\n",
       "      <td>61.167002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.378413</td>\n",
       "      <td>60.273081</td>\n",
       "      <td>59.167749</td>\n",
       "      <td>62.288684</td>\n",
       "      <td>58.127439</td>\n",
       "      <td>59.297788</td>\n",
       "      <td>58.192456</td>\n",
       "      <td>60.273081</td>\n",
       "      <td>60.338104</td>\n",
       "      <td>58.760685</td>\n",
       "      <td>58.526605</td>\n",
       "      <td>59.525472</td>\n",
       "      <td>57.797486</td>\n",
       "      <td>60.160965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.167749</td>\n",
       "      <td>60.728216</td>\n",
       "      <td>60.403121</td>\n",
       "      <td>61.053318</td>\n",
       "      <td>59.037709</td>\n",
       "      <td>59.947985</td>\n",
       "      <td>58.777630</td>\n",
       "      <td>60.338104</td>\n",
       "      <td>58.517557</td>\n",
       "      <td>61.324787</td>\n",
       "      <td>61.937243</td>\n",
       "      <td>59.455687</td>\n",
       "      <td>61.197340</td>\n",
       "      <td>57.142860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Genesis     Exodus  Leviticus    Numbers  Deuteronomy     Joshua  \\\n",
       "0  57.932377  60.663199  61.378413  60.858256    59.362811  59.752923   \n",
       "1  61.378413  60.273081  59.167749  62.288684    58.127439  59.297788   \n",
       "2  59.167749  60.728216  60.403121  61.053318    59.037709  59.947985   \n",
       "\n",
       "      Judges     Samuel      Kings     Esther     Daniel       Ezra  \\\n",
       "0  61.638492  60.208064  59.947985  61.752135  59.686220  58.967203   \n",
       "1  58.192456  60.273081  60.338104  58.760685  58.526605  59.525472   \n",
       "2  58.777630  60.338104  58.517557  61.324787  61.937243  59.455687   \n",
       "\n",
       "    Nehemiah  Chronicles  \n",
       "0  60.606062   61.167002  \n",
       "1  57.797486   60.160965  \n",
       "2  61.197340   57.142860  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accuracy_df.head(3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
