{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Aramaic and Hebrew clauses using sequence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook Aramaic and Hebrew clauses are distinguished, based on a representation of clauses as a sequence of phrase functions or parts of speech. This analysis serves as a validation of the approach in the analysis in which EBH and LBH clauses are distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qvKgIlYUDHeT",
    "outputId": "1358a687-955f-4543-b0a8-c3f5173947c3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import sys, os, csv, collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start TF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "id": "YqK8198MEhzY",
    "outputId": "6c3c09a9-1309-4ed6-c846-98f77f362621"
   },
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prose books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnEdjHQzDHeg"
   },
   "outputs": [],
   "source": [
    "prose = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', '1_Samuel', '2_Samuel', '1_Kings', '2_Kings', 'Jonah', 'Ruth', 'Esther', 'Daniel', 'Ezra', 'Nehemiah', '1_Chronicles', '2_Chronicles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIsrGrhaDHei"
   },
   "source": [
    "A dictionary is made in which EBH and LBH are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXqBLsGHDHei"
   },
   "outputs": [],
   "source": [
    "ebh_lbh_dict = {}\n",
    "\n",
    "ebh = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', '1_Samuel', '2_Samuel', '1_Kings', '2_Kings']\n",
    "lbh = ['Esther', 'Daniel', 'Ezra', 'Nehemiah', '1_Chronicles', '2_Chronicles']\n",
    "for book in ebh:\n",
    "    ebh_lbh_dict[book] = 'ebh'\n",
    "for book in lbh:\n",
    "    ebh_lbh_dict[book] = 'lbh'\n",
    "\n",
    "double_books = {'1_Samuel': 'Samuel', '2_Samuel' : 'Samuel', '2_Kings' : 'Kings', '1_Kings': 'Kings', '1_Chronicles' : 'Chronicles', '2_Chronicles' : 'Chronicles'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_2zurrbDHem"
   },
   "source": [
    "Count N and Q clauses in Aramaic portions of the MT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "FGqkUyZMDHem",
    "outputId": "9678eea7-200f-4f02-eb32-2443f3a4fb69"
   },
   "outputs": [],
   "source": [
    "ara_tot = 0\n",
    "ara_qnd = collections.defaultdict(int)\n",
    "ara_book = collections.defaultdict(int)\n",
    "\n",
    "for cl in F.otype.s('clause'):\n",
    "    \n",
    "    words1 = L.d(cl, 'word')\n",
    "    if F.language.v(words1[0]) == 'Aramaic':\n",
    "        ara_tot += 1\n",
    "        ara_qnd[F.txt.v(cl)[-1]] += 1\n",
    " \n",
    "print(\"total number of Aramaic clauses: \", ara_tot)\n",
    "print(ara_qnd)\n",
    "print(ara_book)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the data from the BHSA. Note that the function has one argument (level), which can have two values: 'word_level' and 'clause_level'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "ECo6VuuHDHer",
    "outputId": "36ac76f5-3a5f-4ca0-f48d-206850551a8d"
   },
   "outputs": [],
   "source": [
    "def extract_data(level):\n",
    "    \n",
    "    # these lists contain all sequences\n",
    "    seq_h = [] # hebrew sequences\n",
    "    seq_a = [] # aramaic sequences\n",
    "    \n",
    "    elem_count = collections.defaultdict(int) # count \n",
    "\n",
    "    max_len = 0 # check what is the length of the longest clause\n",
    "       \n",
    "    for cl in F.otype.s('clause'):\n",
    "        \n",
    "        # Only Q clauses are selected\n",
    "        if F.txt.v(cl)[-1] in {'D','?', 'N'}:\n",
    "            continue\n",
    "    \n",
    "        # poetic sections are removed\n",
    "        bo,ch,ve = T.sectionFromNode(cl)\n",
    "        if bo == 'Genesis' and ch == 49 and 1 < ve < 28:\n",
    "            continue\n",
    "        elif bo == 'Exodus' and ch == 15 and ve < 19:\n",
    "            continue\n",
    "        elif bo == 'Numbers' and ch in {23,24}:\n",
    "            continue\n",
    "        elif bo == 'Deuteronomy' and ch in {32,33}:\n",
    "            continue\n",
    "        elif bo == 'Judges' and ch == 5:\n",
    "            continue\n",
    "        elif bo == '1_Samuel' and ch == 2 and ve < 11:\n",
    "            continue\n",
    "        elif bo == '2_Samuel' and ch == 1 and ve > 18:\n",
    "            continue\n",
    "        elif bo == '2_Samuel' and ch == 22:\n",
    "            continue\n",
    "        elif bo == '2_Samuel' and ch == 23 and ve < 8:\n",
    "            continue\n",
    "        elif bo == 'Daniel' and ch == 2 and 19 < ve < 24:\n",
    "            continue\n",
    "        elif bo == 'Daniel' and ch == 8 and 22 < ve < 27:\n",
    "            continue  \n",
    "        elif bo == 'Daniel' and ch == 12 and ve < 4:\n",
    "            continue\n",
    "        elif bo == 'Nehemiah' and ch == 9 and 5 < ve < 38:\n",
    "            continue    \n",
    "        if bo == '1_Chronicles' and ch == 16 and 7 < ve < 37:\n",
    "            continue\n",
    "        \n",
    "        words = L.d(cl, 'word')\n",
    "    \n",
    "        if level == 'phrase_level':\n",
    "            if F.language.v(words[0]) == 'Hebrew':\n",
    " \n",
    "                if T.bookName(cl) in prose and T.bookName(cl) not in {'Daniel', 'Ezra'}:\n",
    "                    phrases = L.d(cl, 'phrase')\n",
    "                    funcs = [F.function.v(ph) for ph in phrases]\n",
    "            \n",
    "                    for fun in funcs:\n",
    "                        elem_count[fun] += 1\n",
    "                    seq_h.append(funcs)\n",
    "            \n",
    "                    if len(funcs) > max_len:\n",
    "                        max_len = len(funcs)\n",
    "                \n",
    "            elif F.language.v(words[0]) == 'Aramaic':\n",
    "                if T.bookName(cl) in prose:\n",
    "                    phrases = L.d(cl, 'phrase')\n",
    "                    funcs = [F.function.v(ph) for ph in phrases]\n",
    "        \n",
    "                    for fun in funcs:\n",
    "                        elem_count[fun] += 1\n",
    "                    seq_a.append(funcs)\n",
    "            \n",
    "                    if len(funcs) > max_len:\n",
    "                        max_len = len(funcs)\n",
    "\n",
    "        if level == 'word_level':\n",
    "            words = L.d(cl, 'word')\n",
    "            if F.language.v(words[0]) == 'Hebrew':\n",
    " \n",
    "                if T.bookName(cl) in prose and T.bookName(cl) not in {'Daniel', 'Ezra'}:\n",
    "                    poss = [F.sp.v(w) for w in words]\n",
    "            \n",
    "                    for pos in poss:\n",
    "                        elem_count[pos] += 1\n",
    "                    seq_h.append(poss)\n",
    "            \n",
    "                    if len(poss) > max_len:\n",
    "                        max_len = len(poss)\n",
    "                \n",
    "            elif F.language.v(words[0]) == 'Aramaic':\n",
    "                if T.bookName(cl) in prose:\n",
    "                    poss = [F.sp.v(w) for w in words]\n",
    "        \n",
    "                    for pos in poss:\n",
    "                        elem_count[pos] += 1\n",
    "                    seq_a.append(poss)\n",
    "            \n",
    "                    if len(poss) > max_len:\n",
    "                        max_len = len(poss)\n",
    "                    \n",
    "    return(max_len, seq_h, seq_a, elem_count)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dict, called f2int_dict, for converting phrase functions or parts of speech to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Viw7JwXdDHeu",
    "outputId": "73855470-48b1-460f-f007-59772749d1cf"
   },
   "outputs": [],
   "source": [
    "def create_dict(elem_count):\n",
    "\n",
    "    f2int_dict = {}\n",
    "    f_list = []\n",
    "    for value in elem_count.values():\n",
    "        f_list.append(value)\n",
    "    \n",
    "    sorted_freqs = (sorted(f_list, reverse=True))\n",
    "\n",
    "    for key in elem_count.keys():\n",
    "        f2int_dict[key] = sorted_freqs.index(elem_count[key]) + 1\n",
    "        \n",
    "    return(f2int_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences are converted to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ds68Vak2DHew",
    "outputId": "c471419c-7c99-4d62-8522-51bd96c4768d"
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(seq_h, seq_a, f2int_dict):\n",
    "\n",
    "    ints_h = []\n",
    "    ints_a = []\n",
    "\n",
    "    for clause in seq_h:\n",
    "        seq_ints = [f2int_dict[elem] for elem in clause]\n",
    "        ints_h.append(seq_ints)\n",
    "    for clause in seq_a:\n",
    "        seq_ints = [f2int_dict[elem] for elem in clause]\n",
    "        ints_a.append(seq_ints)\n",
    "    \n",
    "    seq_arr_h = np.asarray(ints_h)\n",
    "    seq_arr_a = np.asarray(ints_a)\n",
    "    \n",
    "    return(seq_arr_h, seq_arr_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aoe2opS2hx88"
   },
   "source": [
    "Select 900 clauses from the Hebrew and Aramaic prose portions of the MT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rH7N2tRCDHe2"
   },
   "outputs": [],
   "source": [
    "def prepare_data(phr_ints_h, phr_ints_a):\n",
    "\n",
    "    sel_heb = np.random.choice(phr_ints_h, 900, replace = False)\n",
    "    sel_ara = np.random.choice(phr_ints_a, 900, replace = False)\n",
    "\n",
    "    selected_input = np.concatenate((sel_heb, sel_ara), axis=0)\n",
    "    tar_heb = [0 for elem in sel_heb]\n",
    "    tar_ara = [1 for elem in sel_ara]\n",
    "    selected_targets = np.array(tar_heb + tar_ara)\n",
    "\n",
    "    X_train = sequence.pad_sequences(selected_input, maxlen=max_len)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(X_train, selected_targets, test_size=0.15, random_state=42)\n",
    "\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TA4WwAKCDHfJ",
    "outputId": "ad8702c0-da75-44c3-f4ed-90c5287ea55a"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # choose 'word_level' or 'phrase_level' as argument of function extract_data()\n",
    "    max_len, seq_h, seq_a, elem_count = extract_data('phrase_level')\n",
    "    f2int_dict = create_dict(elem_count)\n",
    "    seq_arr_h, seq_arr_a = convert_to_ints(seq_h, seq_a, f2int_dict)\n",
    "    data_train, data_test, labels_train, labels_test = prepare_data(seq_arr_h, seq_arr_a)\n",
    "  \n",
    "    # define model\n",
    "    top_words = 100\n",
    "    embedding_vector_length = 32\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=max_len))\n",
    "    model.add(LSTM(200, activation = 'relu', return_sequences=True))\n",
    "    model.add(Dropout(0.5)) # dropout is used to prevent overfitting\n",
    "    model.add(LSTM(200, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = Adam(lr=0.0006)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=256)\n",
    "    \n",
    "    scores = model.evaluate(data_test, labels_test, verbose=0)\n",
    "    \n",
    "    predictions.append(scores[1])\n",
    "    \n",
    "    # remove model from memory\n",
    "    K.clear_session()\n",
    "    \n",
    "    # plot history\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nU0-fZ-kDHfP",
    "outputId": "cf0ff61a-5398-4d0c-cffb-708e28222c2d"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvh = open(r\"aram_heb_phrases.csv\", \"w\")\n",
    "\n",
    "header = ['accuracy']\n",
    "csvh.write('{}\\n'.format(','.join(header)))\n",
    "\n",
    "for value in predictions:\n",
    "    \n",
    "    csvh.write('{}\\n'.format(','.join(str(value)))\n",
    "    \n",
    "csvh.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhrFuncsLSTM-HebrewVSAramaic_7_7_2019.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
